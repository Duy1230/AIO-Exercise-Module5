{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.9\n"
     ]
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 0.1*w1**2 + 2*w2**2\n",
    "\n",
    "print(f(3, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_w(W):\n",
    "    \"\"\"\n",
    "    Compute the gradient dw1 and dw2 of f(w1, w2)\n",
    "    Args:\n",
    "        W: np.ndarray, [w1, w2]\n",
    "    Returns:\n",
    "        dW: np.ndarray, [dw1, dw2], array containing the gradients\n",
    "    \"\"\"\n",
    "    dw1 = 0.2*W[0]\n",
    "    dw2 = 4*W[1]\n",
    "    return np.array([dw1, dw2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Function Minimization\n",
    "\n",
    "- $ f(w_1, w_2) = 0.1w_1^2 + 2w_2^2 $\n",
    "- **Giá trị ban đầu:** $ w_1 = -5 $, $ w_2 = -2 $\n",
    "- **Learning rate:** $ \\alpha = 0.4 $\n",
    "- **Epochs:** 2\n",
    "\n",
    "Tính Gradient\n",
    "\n",
    "- $ \\frac{\\partial f}{\\partial w_1} = 0.2w_1 $\n",
    "- $ \\frac{\\partial f}{\\partial w_2} = 4w_2 $\n",
    "\n",
    "Epoch 1\n",
    "\n",
    "Step 1: Tính Gradient tại $ w_1 = -5 $, $ w_2 = -2 $\n",
    "\n",
    "- $ \\frac{\\partial f}{\\partial w_1} = 0.2 \\times -5 = -1 $\n",
    "- $ \\frac{\\partial f}{\\partial w_2} = 4 \\times -2 = -8 $\n",
    "\n",
    "Step 2: Cập nhật trọng số bằng Gradient Descent\n",
    "\n",
    "- $ w_1 = -5 - 0.4 \\times -1 = -5 + 0.4 = -4.6 $\n",
    "- $ w_2 = -2 - 0.4 \\times -8 = -2 + 3.2 = 1.2 $\n",
    "\n",
    "Epoch 2\n",
    "\n",
    "Step 1: Tính Gradient tại $ w_1 = -4.6 $, $ w_2 = 1.2 $\n",
    "\n",
    "- $ \\frac{\\partial f}{\\partial w_1} = 0.2 \\times -4.6 = -0.92 $\n",
    "- $ \\frac{\\partial f}{\\partial w_2} = 4 \\times 1.2 = 4.8 $\n",
    "\n",
    "Step 2: Cập nhật trọng số bằng Gradient Descent\n",
    "\n",
    "- $ w_1 = -4.6 - 0.4 \\times -0.92 = -4.6 + 0.368 = -4.232 $\n",
    "- $ w_2 = 1.2 - 0.4 \\times 4.8 = 1.2 - 1.92 = -0.72 $\n",
    "\n",
    "Vậy\n",
    "\n",
    "- $ w_1 \\approx -4.232 $\n",
    "- $ w_2 \\approx -0.72 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-5., -2.], dtype=float32), array([-4.6,  1.2]), array([-4.232, -0.72 ]), array([-3.89344,  0.432  ]), array([-3.5819648, -0.2592   ]), array([-3.29540762,  0.15552   ]), array([-3.03177501, -0.093312  ]), array([-2.78923301,  0.0559872 ]), array([-2.56609437, -0.03359232]), array([-2.36080682,  0.02015539]), array([-2.17194227, -0.01209324]), array([-1.99818689,  0.00725594]), array([-1.83833194, -0.00435356]), array([-1.69126538,  0.00261214]), array([-1.55596415, -0.00156728]), array([-1.43148702e+00,  9.40369969e-04]), array([-1.31696806e+00, -5.64221981e-04]), array([-1.21161061e+00,  3.38533189e-04]), array([-1.11468176e+00, -2.03119913e-04]), array([-1.02550722e+00,  1.21871948e-04]), array([-9.43466646e-01, -7.31231688e-05]), array([-8.67989314e-01,  4.38739013e-05]), array([-7.98550169e-01, -2.63243408e-05]), array([-7.34666155e-01,  1.57946045e-05]), array([-6.75892863e-01, -9.47676268e-06]), array([-6.21821434e-01,  5.68605761e-06]), array([-5.72075719e-01, -3.41163456e-06]), array([-5.26309662e-01,  2.04698074e-06]), array([-4.84204889e-01, -1.22818844e-06]), array([-4.45468498e-01,  7.36913066e-07]), array([-4.09831018e-01, -4.42147839e-07])]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Gradient descent\n",
    "\n",
    "def sgd(W, dW, lr):\n",
    "    \"\"\"\n",
    "    Update the weights W using the gradient descent update rule\n",
    "    Args:\n",
    "        W: np.ndarray, [w1, w2]\n",
    "        dW: np.ndarray, [dw1, dw2]\n",
    "        lr: float, learning rate\n",
    "    Returns:\n",
    "        W: np.ndarray, [w1, w2]\n",
    "    \"\"\"\n",
    "    W = W - lr*dW\n",
    "    return W\n",
    "def train_pl(optimizer, lr, epochs):\n",
    "    \"\"\"\n",
    "    Find the minimum of f(w1, w2) using the optimizer\n",
    "    Args:\n",
    "        optimizer: function, the optimizer to use\n",
    "        lr: float, learning rate\n",
    "        epochs: int, number of epochs\n",
    "    Returns:\n",
    "        result: list: list of pairs [w1, w2] after each epoch\n",
    "    \"\"\"\n",
    "    # initial point\n",
    "    W = np.array([-5, -2], dtype=np.float32)\n",
    "\n",
    "    # list of results\n",
    "    result = [W]\n",
    "\n",
    "    # loop over epochs\n",
    "    for _ in range(epochs):\n",
    "        dW = df_w(W)\n",
    "        W = optimizer(W, dW, lr)\n",
    "        result.append(W)\n",
    "    return result\n",
    "\n",
    "print(train_pl(sgd, 0.4, 30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Momentum for Function Minimization\n",
    "\n",
    "- $ f(w_1, w_2) = 0.1w_1^2 + 2w_2^2 $\n",
    "- **Giá trị ban đầu:** $ w_1 = -5 $, $ w_2 = -2 $\n",
    "- **Learning rate:** $ \\alpha = 0.6 $\n",
    "- **Beta:** $ \\beta = 0.5 $\n",
    "- **Vận tốc ban đầu:** $ V_t = [v_1, v_2] = [0, 0] $\n",
    "- **Epochs:** 2\n",
    "\n",
    "Tính Gradient\n",
    "\n",
    "- $ \\frac{\\partial f}{\\partial w_1} = 0.2w_1 $\n",
    "- $ \\frac{\\partial f}{\\partial w_2} = 4w_2 $\n",
    "\n",
    "Epoch 1\n",
    "\n",
    "Step 1: Tính Gradient tại $ w_1 = -5 $, $ w_2 = -2 $\n",
    "\n",
    "- $ \\frac{\\partial f}{\\partial w_1} = 0.2 \\times -5 = -1 $\n",
    "- $ \\frac{\\partial f}{\\partial w_2} = 4 \\times -2 = -8 $\n",
    "\n",
    "Step 2: Tính Vận tốc $ V_t $ bằng Momentum\n",
    "\n",
    "- $ V_t = \\beta V_{t-1} + (1 - \\beta) \\times dW_t $\n",
    "- $ V_1 = 0.5 \\times [0, 0] + (1 - 0.5) \\times [-1, -8] $\n",
    "- $ V_1 = [0, 0] + [0.5 \\times -1, 0.5 \\times -8] $\n",
    "- $ V_1 = [-0.5, -4] $\n",
    "\n",
    "Step 3: Cập nhật trọng số bằng Gradient Descent with Momentum\n",
    "\n",
    "- $ W_t = W_{t-1} - \\alpha \\times V_t $\n",
    "- $ w_1 = -5 - 0.6 \\times -0.5 = -5 + 0.3 = -4.7 $\n",
    "- $ w_2 = -2 - 0.6 \\times -4 = -2 + 2.4 = 0.4 $\n",
    "\n",
    "Epoch 2\n",
    "\n",
    "Step 1: Tính Gradient tại $ w_1 = -4.7 $, $ w_2 = 0.4 $\n",
    "\n",
    "- $ \\frac{\\partial f}{\\partial w_1} = 0.2 \\times -4.7 = -0.94 $\n",
    "- $ \\frac{\\partial f}{\\partial w_2} = 4 \\times 0.4 = 1.6 $\n",
    "\n",
    "Step 2: Tính Vận tốc $ V_t $ bằng Momentum\n",
    "\n",
    "- $ V_t = \\beta V_{t-1} + (1 - \\beta) \\times dW_t $\n",
    "- $ V_2 = 0.5 \\times [-0.5, -4] + (1 - 0.5) \\times [-0.94, 1.6] $\n",
    "- $ V_2 = [-0.25, -2] + [0.5 \\times -0.94, 0.5 \\times 1.6] $\n",
    "- $ V_2 = [-0.25, -2] + [-0.47, 0.8] $\n",
    "- $ V_2 = [-0.72, -1.2] $\n",
    "\n",
    "Step 3: Cập nhật trọng số bằng Gradient Descent with Momentum\n",
    "\n",
    "- $ W_t = W_{t-1} - \\alpha \\times V_t $\n",
    "- $ w_1 = -4.7 - 0.6 \\times -0.72 = -4.7 + 0.432 = -4.268 $\n",
    "- $ w_2 = 0.4 - 0.6 \\times -1.2 = 0.4 + 0.72 = 1.12 $\n",
    "\n",
    "\n",
    "Kết quả:\n",
    "\n",
    "- $ w_1 \\approx -4.268 $\n",
    "- $ w_2 \\approx 1.12 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-5., -2.], dtype=float32), array([-4.7,  0.4]), array([-4.268,  1.12 ]), array([-3.79592,  0.136  ]), array([-3.3321248, -0.5192   ]), array([-2.90029971, -0.22376   ]), array([-2.51036919,  0.192472  ]), array([-2.16478177,  0.1696216 ]), array([-1.86210116, -0.04534952]), array([-1.59903478, -0.09841566]), array([-1.37155951, -0.00684994]), array([-1.1755283 ,  0.04715285]), array([-1.006981  ,  0.01757082]), array([-0.86228849, -0.01830518]), array([-0.73820492, -0.01427696]), array([-0.63187084,  0.0048695 ]), array([-0.54079155,  0.00859933]), array([-4.62804416e-01,  1.45050014e-04]), array([-0.39604258, -0.00425615]), array([-0.33889911, -0.00134937]), array([-0.28999343,  0.00172326]), array([-0.24814098,  0.00119166]), array([-0.2123263 , -0.00050413]), array([-0.18167938, -0.00074707]), array([-1.55455157e-01,  2.79448010e-05]), array([-0.13301574,  0.00038192]), array([-1.13815082e-01,  1.00603444e-04]), array([-0.09738585, -0.00016078]), array([-8.33280829e-02, -9.85353344e-05]), array([-7.12995144e-02,  5.08287536e-05]), array([-6.10072592e-02,  6.45162933e-05])]\n"
     ]
    }
   ],
   "source": [
    "def sgd_momentum(W, V, dW, lr, gamma=0.5):\n",
    "    \"\"\"\n",
    "    Update the weights W using the gradient descent with momentum update rule\n",
    "    Args:\n",
    "        W: np.ndarray, [w1, w2]\n",
    "        V: np.ndarray, [v1, v2]\n",
    "        dW: np.ndarray, [dw1, dw2]\n",
    "        lr: float, learning rate\n",
    "        gamma: float, momentum\n",
    "    Returns:\n",
    "        V: np.ndarray, [v1, v2]\n",
    "        W: np.ndarray, [w1, w2]\n",
    "    \"\"\"\n",
    "    V = gamma*V + (1-gamma)*dW\n",
    "    W = W - lr*V\n",
    "    return W, V\n",
    "\n",
    "def train_pl(optimizer, lr, epochs):\n",
    "    \"\"\"\n",
    "    Find the minimum of f(w1, w2) using the optimizer\n",
    "    Args:\n",
    "        optimizer: function, the optimizer to use\n",
    "        lr: float, learning rate\n",
    "        epochs: int, number of epochs\n",
    "    Returns:\n",
    "        result: list: list of pairs [w1, w2] after each epoch\n",
    "    \"\"\"\n",
    "    # initial point\n",
    "    W = np.array([-5, -2], dtype=np.float32)\n",
    "    V = np.array([0, 0], dtype=np.float32)\n",
    "    # list of results\n",
    "    result = [W]\n",
    "\n",
    "    # loop over epochs\n",
    "    for _ in range(epochs):\n",
    "        dW = df_w(W)\n",
    "        W, V = optimizer(W, V, dW, lr)\n",
    "        result.append(W)\n",
    "    return result\n",
    "\n",
    "print(train_pl(sgd_momentum, 0.6, 30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp Optimization for Function Minimization\n",
    "\n",
    "\n",
    "- $ f(w_1, w_2) = 0.1w_1^2 + 2w_2^2 $\n",
    "- **Giá trị khởi đầu:** $ w_1 = -5 $, $ w_2 = -2 $\n",
    "- **Giá trị khởi đầu của squared gradients:** $ s_1 = 0 $, $ s_2 = 0 $\n",
    "- **Learning rate:** $ \\alpha = 0.3 $\n",
    "- **Decay rate:** $ \\gamma = 0.9 $\n",
    "- **Epsilon:** $ \\epsilon = 10^{-6} $\n",
    "- **Epochs:** 2\n",
    "\n",
    "Tính Gradient\n",
    "\n",
    "- $ \\frac{\\partial f}{\\partial w_1} = 0.2w_1 $\n",
    "- $ \\frac{\\partial f}{\\partial w_2} = 4w_2 $\n",
    "\n",
    "Epoch 1\n",
    "\n",
    "Step 1: Tính Gradient tại $ w_1 = -5 $, $ w_2 = -2 $\n",
    "\n",
    "- $ \\frac{\\partial f}{\\partial w_1} = 0.2 \\times -5 = -1 $\n",
    "- $ \\frac{\\partial f}{\\partial w_2} = 4 \\times -2 = -8 $\n",
    "\n",
    "Step 2: Cập nhật squared gradients $ s_1 $ và $ s_2 $\n",
    "\n",
    "- $ s_1 = \\gamma s_1 + (1 - \\gamma) \\times (\\frac{\\partial f}{\\partial w_1})^2 $\n",
    "- $ s_1 = 0.9 \\times 0 + 0.1 \\times (-1)^2 = 0.1 $\n",
    "- $ s_2 = \\gamma s_2 + (1 - \\gamma) \\times (\\frac{\\partial f}{\\partial w_2})^2 $\n",
    "- $ s_2 = 0.9 \\times 0 + 0.1 \\times (-8)^2 = 6.4 $\n",
    "\n",
    "Step 3: Cập nhật trọng số bằng RMSProp\n",
    "\n",
    "- $ w_1 = w_1 - \\alpha \\times \\frac{\\frac{\\partial f}{\\partial w_1}}{\\sqrt{s_1 + \\epsilon}} $\n",
    "- $ w_1 = -5 - 0.3 \\times \\frac{-1}{\\sqrt{0.1 + 10^{-6}}} \\approx -4.0513 $\n",
    "- $ w_2 = w_2 - \\alpha \\times \\frac{\\frac{\\partial f}{\\partial w_2}}{\\sqrt{s_2 + \\epsilon}} $\n",
    "- $ w_2 = -2 - 0.3 \\times \\frac{-8}{\\sqrt{6.4 + 10^{-6}}} \\approx -1.0513 $\n",
    "\n",
    "Epoch 2\n",
    "\n",
    "Step 1: Tính Gradient tại $ w_1 = -4.0513 $, $ w_2 = -1.0513 $\n",
    "\n",
    "- $ \\frac{\\partial f}{\\partial w_1} = 0.2 \\times -4.0513 \\approx -0.8103 $\n",
    "- $ \\frac{\\partial f}{\\partial w_2} = 4 \\times -1.0513 \\approx -4.2053 $\n",
    "\n",
    "Step 2: Cập nhật squared gradients $ s_1 $ và $ s_2 $\n",
    "\n",
    "- $ s_1 = \\gamma s_1 + (1 - \\gamma) \\times (\\frac{\\partial f}{\\partial w_1})^2 $\n",
    "- $ s_1 = 0.9 \\times 0.1 + 0.1 \\times (-0.8103)^2 \\approx 0.1556 $\n",
    "- $ s_2 = \\gamma s_2 + (1 - \\gamma) \\times (\\frac{\\partial f}{\\partial w_2})^2 $\n",
    "- $ s_2 = 0.9 \\times 6.4 + 0.1 \\times (-4.2053)^2 \\approx 7.5284 $\n",
    "\n",
    "Step 3: Cập nhật trọng số bằng RMSProp\n",
    "\n",
    "- $ w_1 = w_1 - \\alpha \\times \\frac{\\frac{\\partial f}{\\partial w_1}}{\\sqrt{s_1 + \\epsilon}} $\n",
    "- $ w_1 = -4.0513 - 0.3 \\times \\frac{-0.8103}{\\sqrt{0.1556 + 10^{-6}}} \\approx -3.4351 $\n",
    "- $ w_2 = w_2 - \\alpha \\times \\frac{\\frac{\\partial f}{\\partial w_2}}{\\sqrt{s_2 + \\epsilon}} $\n",
    "- $ w_2 = -1.0513 - 0.3 \\times \\frac{-4.2053}{\\sqrt{7.5284 + 10^{-6}}} \\approx -0.5920 $\n",
    "\n",
    "Kết quả:\n",
    "\n",
    "- $ w_1 \\approx -3.4351 $\n",
    "- $ w_2 \\approx -0.5920 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-5., -2.], dtype=float32), array([-4.05132145, -1.05131678]), array([-3.43519754, -0.59152343]), array([-2.95893693, -0.3294394 ]), array([-2.56546289, -0.17756482]), array([-2.22920552, -0.09163256]), array([-1.93626752, -0.04494499]), array([-1.67817686, -0.02081423]), array([-1.44934985, -0.00903559]), array([-1.24588199, -0.00364591]), array([-1.06490301, -0.00135351]), array([-9.04202260e-01, -4.56444431e-04]), array([-7.61996495e-01, -1.37562928e-04]), array([-6.36778499e-01, -3.62601019e-05]), array([-5.27215237e-01, -8.11337456e-06]), array([-4.32078505e-01, -1.47473412e-06]), array([-3.50198507e-01, -2.02783991e-07]), array([-2.80434649e-01, -1.84231187e-08]), array([-2.21659834e-01, -7.67742748e-10]), array([-1.72755512e-01,  7.80451998e-12]), array([-1.32615134e-01, -5.05794800e-13]), array([-1.00153779e-01,  6.19123501e-14]), array([-7.43217708e-02, -1.13373781e-14]), array([-5.41201278e-02,  2.80166702e-15]), array([-3.86159157e-02, -8.81341191e-16]), array([-2.69558066e-02,  3.39921117e-16]), array([-1.83765633e-02, -1.56581731e-16]), array([-1.22116093e-02,  8.44994985e-17]), array([-7.89331794e-03, -5.26376595e-17]), array([-4.95110261e-03,  3.74107995e-17]), array([-3.00577081e-03, -3.00506084e-17])]\n"
     ]
    }
   ],
   "source": [
    "def RMSprop(W, S, dW, lr, gamma=0.9, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Update the weights W using RMSprop\n",
    "    Args:\n",
    "        W: np.ndarray, [w1, w2]\n",
    "        S: np.ndarray, [s1, s2]\n",
    "        dW: np.ndarray, [dw1, dw2]\n",
    "        lr: float, learning rate\n",
    "        gamma: float, decay rate \n",
    "        epsilon: float\n",
    "    Returns:\n",
    "        S: np.ndarray, [s1, s2],\n",
    "        W: np.ndarray, [w1, w2], updated weights\n",
    "    \"\"\"\n",
    "    S = gamma*S + (1- gamma)*dW**2\n",
    "    W = W - lr* dW/(np.sqrt(S + epsilon))\n",
    "    return W, S\n",
    "\n",
    "\n",
    "def train_pl(optimizer, lr, epochs):\n",
    "    \"\"\"\n",
    "    Find the minimum of f(w1, w2) using the optimizer\n",
    "    Args:\n",
    "        optimizer: function, the optimizer to use\n",
    "        lr: float, learning rate\n",
    "        epochs: int, number of epochs\n",
    "    Returns:\n",
    "        result: list: list of pairs [w1, w2] after each epoch\n",
    "    \"\"\"\n",
    "    # initial point\n",
    "    W = np.array([-5, -2], dtype=np.float32)\n",
    "    S = np.array([0, 0], dtype=np.float32)\n",
    "    # list of results\n",
    "    result = [W]\n",
    "\n",
    "    # loop over epochs\n",
    "    for _ in range(epochs):\n",
    "        dW = df_w(W)\n",
    "        W, S = optimizer(W, S, dW, lr)\n",
    "        result.append(W)\n",
    "    return result\n",
    "\n",
    "print(train_pl(RMSprop, 0.3, 30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-5., -2.], dtype=float32), array([-4.8000002 , -1.80000002]), array([-4.60025478, -1.60082451]), array([-4.40094848, -1.40317262]), array([-4.20227764, -1.20787822]), array([-4.00445033, -1.01592745]), array([-3.80768638, -0.82847307]), array([-3.61221732, -0.64684159]), array([-3.41828623, -0.47252765]), array([-3.22614739, -0.30716934]), array([-3.03606592, -0.15249855]), array([-2.84831706, -0.01026326]), array([-2.66318543,  0.11787552]), array([-2.480964  ,  0.23046161]), array([-2.30195279,  0.3263587 ]), array([-2.12645742,  0.40484195]), array([-1.95478732,  0.46564961]), array([-1.7872537 ,  0.50898799]), array([-1.62416726,  0.53549442]), array([-1.46583566,  0.54617144]), array([-1.31256067,  0.54230812]), array([-1.16463526,  0.52540206]), array([-1.02234036,  0.4970906 ]), array([-0.88594163,  0.4590951 ]), array([-0.75568617,  0.41317781]), array([-0.63179919,  0.3611089 ]), array([-0.51448089,  0.30464048]), array([-0.40390346,  0.24548409]), array([-0.30020842,  0.18528918]), array([-0.20350426,  0.12562074]), array([-0.11386457,  0.06793529])]\n"
     ]
    }
   ],
   "source": [
    "def adam(W, V, S, dW, lr, t, beta1=0.9, beta2=0.999, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Update the weights W using the Adam optimizer\n",
    "    Args:\n",
    "        W: np.ndarray, [w1, w2]\n",
    "        V: np.ndarray, [v1, v2]\n",
    "        S: np.ndarray, [s1, s2]\n",
    "        dW: np.ndarray, [dw1, dw2]\n",
    "        lr: float, learning rate\n",
    "        t: int, time step\n",
    "        beta1: float, decay rate for the first moment\n",
    "        beta2: float, decay rate for the second moment\n",
    "        epsilon: float, small constant to prevent division by zero\n",
    "    Returns:\n",
    "        W: np.ndarray, [w1, w2], updated weights\n",
    "        V: np.ndarray, [v1, v2], updated first moment\n",
    "        S: np.ndarray, [s1, s2], updated second moment\n",
    "    \"\"\"\n",
    "    V = beta1 * V + (1 - beta1) * dW\n",
    "    S = beta2 * S + (1 - beta2) * (dW ** 2)\n",
    "    V_corr = V / (1 - beta1 ** (t+1))\n",
    "    S_corr = S / (1 - beta2 ** (t+1))\n",
    "    W = W - lr * V_corr / (np.sqrt(S_corr) + epsilon)\n",
    "    return W, V, S\n",
    "\n",
    "def train_pl(optimizer, lr, epochs):\n",
    "    W = np.array([-5, -2], dtype=np.float32)\n",
    "    V = np.array([0, 0], dtype=np.float32)\n",
    "    S = np.array([0, 0], dtype=np.float32)\n",
    "    result = [W]\n",
    "\n",
    "    for t in range(epochs):\n",
    "        dW = df_w(W)\n",
    "        W, V, S = optimizer(W, V, S, dW, lr, t)\n",
    "        result.append(W)\n",
    "    return result\n",
    "\n",
    "print(train_pl(adam, 0.2, 30))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
